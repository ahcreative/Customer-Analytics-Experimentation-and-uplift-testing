# -*- coding: utf-8 -*-
"""Task 1: Data preparation and customer analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GBesl6LQCVlM_kyTaakhZMw5GHfjdpVw
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind

# Loading required libraries and datasets
transaction_data = pd.read_csv("/content/transaction_data.csv")
customer_data = pd.read_csv("/content/QVI_purchase_behaviour.csv")

# Exploratory data analysis
# Examining transaction data
print(transaction_data.head())  # View first few rows of transaction data
print(transaction_data.info())  # Summary of transaction data

# Droping the rows with PROD_QTY = 0 or missing values
transaction_data.drop(transaction_data[(transaction_data['PROD_QTY'] == 0) | (transaction_data['PROD_QTY'].isnull())].index, inplace=True)

# Filling the missing values in TOT_SALES column with mean
transaction_data.fillna(transaction_data['TOT_SALES'].mean(), inplace=True)

# Converting the DATE column to date format
transaction_data['DATE'] = pd.to_datetime(transaction_data['DATE'], origin='1899-12-30', unit='D')
print(transaction_data.info())

# Summary of PROD_NAME
print(transaction_data['PROD_NAME'].describe())
print('Hello world')

# Further examine PROD_NAME
product_words = transaction_data['PROD_NAME'].str.split(expand=True).stack().str.replace(r'\d+', '').str.replace(r'[^\w\s]', '')
word_counts = product_words.value_counts().reset_index()
word_counts.columns = ['Word', 'Count']
print(word_counts)

# Removing the salsa products
transaction_data = transaction_data[~transaction_data['PROD_NAME'].str.contains('salsa', case=False)]

# Initial summary
print(transaction_data.describe())

# Filtering the dataset to find the outlier
outlier_transactions = transaction_data[transaction_data['PROD_QTY'] == 200]
print(outlier_transactions)

# Removing the outlier customer
transaction_data = transaction_data[transaction_data['LYLTY_CARD_NBR'] < 300000]

# Re-examine transaction data
print(transaction_data.describe())

# Counting the number of transactions by date
transaction_counts = transaction_data.groupby('DATE').size()
print(transaction_counts.head())

# Ploting transactions over time
plt.figure(figsize=(10, 6))
transaction_counts.plot()
plt.title('Transactions Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Transactions')
plt.show()

# Creating pack size
transaction_data['PACK_SIZE'] = transaction_data['PROD_NAME'].str.extract(r'(\d+)[gG]')

# Ploting a histogram of PACK_SIZE
plt.figure(figsize=(10, 6))
transaction_data['PACK_SIZE'].astype(float).hist(bins=20)
plt.title('Distribution of Pack Sizes')
plt.xlabel('Pack Size (g)')
plt.ylabel('Frequency')
plt.show()

# Creating brand name
transaction_data['BRAND'] = transaction_data['PROD_NAME'].str.split().str[0]

# Cleaning brand names
transaction_data.loc[transaction_data['BRAND'] == 'Red', 'BRAND'] = 'RRD'

# Examining customer data
# Merging transaction data to customer data
transaction_data = pd.merge(transaction_data, customer_data, on='LYLTY_CARD_NBR', how='left')

# Checking for missing customer details
missing_customers = transaction_data[transaction_data['LIFESTAGE'].isnull()]
print(missing_customers)

# Data analysis on customer segments
# Total sales by LIFESTAGE and PREMIUM_CUSTOMER
sales_by_segment = transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['TOT_SALES'].sum().reset_index()
print(sales_by_segment.head())

# Preparing data for plotting
sales_by_segment = transaction_data.pivot_table(index=['LIFESTAGE', 'PREMIUM_CUSTOMER'], values='TOT_SALES', aggfunc='sum')
sales_by_segment.reset_index(inplace=True)
plt.figure(figsize=(10, 6))
for premium_customer, group_data in sales_by_segment.groupby('PREMIUM_CUSTOMER'):
    plt.bar(group_data['LIFESTAGE'], group_data['TOT_SALES'], label=premium_customer)

plt.title('Proportion of Sales by Lifestage and Premium Customer')
plt.xlabel('Lifestage')
plt.ylabel('Total Sales')
plt.xticks(rotation=90)
plt.legend(title='Premium Customer')
plt.tight_layout()
plt.show()

# Number of customers by LIFESTAGE and PREMIUM_CUSTOMER
customer_counts = transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['LYLTY_CARD_NBR'].nunique().reset_index()
print(customer_counts.head())

# Average number of units per customer by LIFESTAGE and PREMIUM_CUSTOMER
units_per_customer = transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['PROD_QTY'].sum() / \
            transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['LYLTY_CARD_NBR'].nunique()

units_per_customer = units_per_customer.reset_index().sort_values(by=0, ascending=False)

fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(x='LIFESTAGE', y=0, hue='PREMIUM_CUSTOMER', data=units_per_customer, ax=ax)

ax.set_xlabel('Lifestage')
ax.set_ylabel('Avg Units per Transaction')
ax.set_title('Units per Customer')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Average number of sales per customer by LIFESTAGE and PREMIUM_CUSTOMER
sales_per_customer = transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER'])['TOT_SALES'].mean().reset_index()
print(sales_per_customer.head())

avg_price = transaction_data.groupby(['LIFESTAGE', 'PREMIUM_CUSTOMER']).apply(lambda x: x['TOT_SALES'].sum() / x['PROD_QTY'].sum()).reset_index(name='AVG')
avg_price = avg_price.sort_values(by='AVG', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(data=avg_price, x='LIFESTAGE', y='AVG', hue='PREMIUM_CUSTOMER', palette='viridis')
plt.title('Average Price per Unit by Lifestage and Premium Customer')
plt.xlabel('Lifestage')
plt.ylabel('Avg Price per Unit')
plt.xticks(rotation=90)
plt.legend(title='Premium Customer')
plt.tight_layout()
plt.show()

# Creating a new column 'price' with the price per unit
transaction_data['price'] = transaction_data['TOT_SALES'] / transaction_data['PROD_QTY']

# Filtering the data for the relevant segments
young_midage = transaction_data[(transaction_data['LIFESTAGE'].isin(['YOUNG SINGLES/COUPLES', 'MIDAGE SINGLES/COUPLES']))]

# Separating the data into mainstream and non-mainstream groups
mainstream_group = young_midage[young_midage['PREMIUM_CUSTOMER'] == 'Mainstream']['price']
non_mainstream_group = young_midage[young_midage['PREMIUM_CUSTOMER'] != 'Mainstream']['price']

# Performing the independent t-test
t_stat, p_val = ttest_ind(mainstream_group, non_mainstream_group, alternative='greater')

print(f"T-statistic: {t_stat}")
print(f"P-value: {p_val}")

# Deep diving into Mainstream, young singles/couples segment
segment1 = transaction_data[(transaction_data['LIFESTAGE'] == "YOUNG SINGLES/COUPLES") & (transaction_data['PREMIUM_CUSTOMER'] == "Mainstream")]
other = transaction_data[~((transaction_data['LIFESTAGE'] == "YOUNG SINGLES/COUPLES") & (transaction_data['PREMIUM_CUSTOMER'] == "Mainstream"))]

# Brand affinity compared to the rest of the population
quantity_segment1 = segment1['PROD_QTY'].sum()
quantity_other = other['PROD_QTY'].sum()
quantity_segment1_by_brand = segment1.groupby('BRAND')['PROD_QTY'].sum() / quantity_segment1
quantity_segment1_by_brand = quantity_segment1_by_brand.reset_index().rename(columns={'PROD_QTY': 'targetSegment'})
quantity_other_by_brand = other.groupby('BRAND')['PROD_QTY'].sum() / quantity_other
quantity_other_by_brand = quantity_other_by_brand.reset_index().rename(columns={'PROD_QTY': 'other'})
brand_proportions = pd.merge(quantity_segment1_by_brand, quantity_other_by_brand, on='BRAND', how='outer')
brand_proportions['affinityToBrand'] = brand_proportions['targetSegment'] / brand_proportions['other']
brand_proportions = brand_proportions.fillna(0)  # Replace NaN with 0
brand_proportions = brand_proportions.sort_values(by='affinityToBrand', ascending=False)
print(brand_proportions)

# Preferred pack size compared to the rest of the population
quantity_segment1_by_pack = segment1.groupby('PACK_SIZE')['PROD_QTY'].sum() / quantity_segment1
quantity_segment1_by_pack = quantity_segment1_by_pack.reset_index().rename(columns={'PROD_QTY': 'targetSegment'})
quantity_other_by_pack = other.groupby('PACK_SIZE')['PROD_QTY'].sum() / quantity_other
quantity_other_by_pack = quantity_other_by_pack.reset_index().rename(columns={'PROD_QTY': 'other'})
pack_proportions = pd.merge(quantity_segment1_by_pack, quantity_other_by_pack, on='PACK_SIZE', how='outer')
pack_proportions['affinityToPack'] = pack_proportions['targetSegment'] / pack_proportions['other']
pack_proportions = pack_proportions.fillna(0)  # Replace NaN with 0
pack_proportions = pack_proportions.sort_values(by='affinityToPack', ascending=False)
print(pack_proportions)

# Deep diving into specific customer segments for insights
mainstream_young = transaction_data[(transaction_data['LIFESTAGE'] == 'YOUNG SINGLES/COUPLES') &
                                (transaction_data['PREMIUM_CUSTOMER'] == 'Mainstream')]

# Preferred brand
brand_preference = mainstream_young['BRAND'].value_counts().reset_index()
print(brand_preference)

# Preferred pack size
pack_size_preference = mainstream_young['PACK_SIZE'].value_counts().reset_index()
print(pack_size_preference)

# Save the merged data to CSV
transaction_data.to_csv("/content/merged_data.csv", index=False)