# -*- coding: utf-8 -*-
"""Experimentation and uplift testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EPfDD536VNSGfUTy4KItw6tPvvYl8Lws
"""

import pandas as pd
import numpy as np
from datetime import datetime

# Load data
data = pd.read_csv("/content/QVI_data (2).csv")

# Create a month ID
data['DATE'] = pd.to_datetime(data['DATE'])
data['YEARMONTH'] = data['DATE'].dt.year * 100 + data['DATE'].dt.month

# Define the measure calculations
measure_over_time = data.groupby(['STORE_NBR', 'YEARMONTH']).\
    agg({'TOT_SALES': 'sum',
         'LYLTY_CARD_NBR': pd.Series.nunique,
         'TXN_ID': pd.Series.nunique,
         'PROD_QTY': 'sum'}).\
    assign(nTxnPerCust=lambda x: x['TXN_ID'] / x['LYLTY_CARD_NBR'],
           avgPricePerUnit=lambda x: x['TOT_SALES'] / x['PROD_QTY']).\
    reset_index().\
    sort_values(['STORE_NBR', 'YEARMONTH'])

# Filter to the pre-trial period and stores with full observation periods
stores_with_full_obs = measure_over_time.groupby('STORE_NBR').filter(lambda x: len(x) == 12)['STORE_NBR'].unique()
pre_trial_measures = measure_over_time.loc[(measure_over_time['YEARMONTH'] < 201902) &
                                           (measure_over_time['STORE_NBR'].isin(stores_with_full_obs))]

# Create a function to calculate correlation for a measure, looping through each control store
def calculate_correlation(input_table, metric_col, store_comparison):
    calc_corr_table = pd.DataFrame(columns=['Store1', 'Store2', 'corr_measure'])
    store_numbers = input_table['STORE_NBR'].unique()
    for i in store_numbers:
        calculated_measure = pd.DataFrame({'Store1': [store_comparison], 'Store2': [i],
                                           'corr_measure': [input_table.loc[input_table['STORE_NBR'] == store_comparison, metric_col].corr(input_table.loc[input_table['STORE_NBR'] == i, metric_col])]})
        calc_corr_table = pd.concat([calc_corr_table, calculated_measure], ignore_index=True)
    return calc_corr_table

# Create a function to calculate a standardized magnitude distance for a measure, looping through each control store
def calculate_magnitude_distance(input_table, metric_col, store_comparison):
    calc_dist_table = pd.DataFrame(columns=['Store1', 'Store2', 'YEARMONTH', 'measure'])
    store_numbers = input_table['STORE_NBR'].unique()
    for i in store_numbers:
        comparison_data = input_table[input_table['STORE_NBR'] == store_comparison]
        i_data = input_table[input_table['STORE_NBR'] == i]

        # Ensure that we consider only the common months between the trial store and the control store
        common_months = comparison_data.merge(i_data, on='YEARMONTH', how='inner')['YEARMONTH'].unique()
        common_data_comparison = comparison_data[comparison_data['YEARMONTH'].isin(common_months)]
        common_data_i = i_data[i_data['YEARMONTH'].isin(common_months)]

        calculated_measure = pd.DataFrame({
            'Store1': [store_comparison] * len(common_data_comparison),
            'Store2': [i] * len(common_data_comparison),
            'YEARMONTH': common_data_comparison['YEARMONTH'],
            'measure': np.abs(common_data_comparison[metric_col].values - common_data_i[metric_col].values)
        })
        calc_dist_table = pd.concat([calc_dist_table, calculated_measure], ignore_index=True)

    # Standardize the magnitude distance so that the measure ranges from 0 to 1
    min_max_dist = calc_dist_table.groupby(['Store1', 'YEARMONTH'])['measure'].agg(['min', 'max']).reset_index()
    dist_table = pd.merge(calc_dist_table, min_max_dist, on=['Store1', 'YEARMONTH'])
    dist_table['magnitudeMeasure'] = 1 - (dist_table['measure'] - dist_table['min']) / (dist_table['max'] - dist_table['min'])
    final_dist_table = dist_table.groupby(['Store1', 'Store2'])['magnitudeMeasure'].mean().reset_index()
    return final_dist_table



# Trial store 77
trial_store = 77

# Use the functions for calculating correlation
corr_n_sales = calculate_correlation(pre_trial_measures, 'TOT_SALES', trial_store)
corr_n_customers = calculate_correlation(pre_trial_measures, 'LYLTY_CARD_NBR', trial_store)

# Use the functions for calculating magnitude
magnitude_n_sales = calculate_magnitude_distance(pre_trial_measures, 'TOT_SALES', trial_store)
magnitude_n_customers = calculate_magnitude_distance(pre_trial_measures, 'LYLTY_CARD_NBR', trial_store)

# Create a combined score composed of correlation and magnitude
# Create a combined score composed of correlation and magnitude
corr_weight = 0.5
score_n_sales = pd.merge(corr_n_sales, magnitude_n_sales, on=['Store1', 'Store2'])
score_n_sales['scoreNSales'] = score_n_sales['corr_measure'] * corr_weight + score_n_sales['magnitudeMeasure'] * (1 - corr_weight)

score_n_customers = pd.merge(corr_n_customers, magnitude_n_customers, on=['Store1', 'Store2'])
score_n_customers['scoreNCust'] = score_n_customers['corr_measure'] * corr_weight + score_n_customers['magnitudeMeasure'] * (1 - corr_weight)


# Combine scores across the drivers
score_control = pd.merge(score_n_sales, score_n_customers, on=['Store1', 'Store2'])
score_control['finalControlScore'] = score_control['scoreNSales'] * 0.5 + score_control['scoreNCust'] * 0.5

# Select control store based on the highest matching store (closest to 1 but not the store itself)
score_control['finalControlScore'] = pd.to_numeric(score_control['finalControlScore'], errors='coerce')
control_store = score_control.loc[score_control['Store1'] == trial_store, 'finalControlScore'].nlargest(2).iloc[1]

# Visual checks on trends based on the drivers
measure_over_time_sales = measure_over_time.copy()
past_sales = measure_over_time_sales.assign(Store_type=lambda x: np.where(x['STORE_NBR'] == trial_store, 'Trial',
                                                                           np.where(x['STORE_NBR'] == control_store, 'Control', 'Other stores')))
past_sales = past_sales.loc[past_sales['YEARMONTH'] < 201903, ['YEARMONTH', 'Store_type', 'TOT_SALES']].groupby(['YEARMONTH', 'Store_type'])['TOT_SALES'].mean().reset_index()
past_sales['TransactionMonth'] = pd.to_datetime(dict(year=past_sales['YEARMONTH'] // 100, month=past_sales['YEARMONTH'] % 100, day=1))

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
for store_type, group in past_sales.groupby('Store_type'):
    plt.plot(group['TransactionMonth'], group['TOT_SALES'], label=store_type)
plt.title('Total sales by month')
plt.xlabel('Month of operation')
plt.ylabel('Total sales')
plt.legend()
plt.show()

# Visual checks on trends based on the drivers
measure_over_time_custs = measure_over_time.copy()
past_customers = measure_over_time_custs.assign(Store_type=lambda x: np.where(x['STORE_NBR'] == trial_store, 'Trial',
                                                                               np.where(x['STORE_NBR'] == control_store, 'Control', 'Other stores')))
past_customers = past_customers.loc[past_customers['YEARMONTH'] < 201903, ['YEARMONTH', 'Store_type', 'LYLTY_CARD_NBR']].groupby(['YEARMONTH', 'Store_type'])['LYLTY_CARD_NBR'].mean().reset_index()
past_customers['TransactionMonth'] = pd.to_datetime(dict(year=past_customers['YEARMONTH'] // 100, month=past_customers['YEARMONTH'] % 100, day=1))

plt.figure(figsize=(10, 6))
for store_type, group in past_customers.groupby('Store_type'):
    plt.plot(group['TransactionMonth'], group['LYLTY_CARD_NBR'], label=store_type)
plt.title('Total number of customers by month')
plt.xlabel('Month of operation')
plt.ylabel('Total number of customers')
plt.legend()
plt.show()

# Scale pre-trial control sales to match pre-trial trial store sales
scaling_factor_for_control_sales = pre_trial_measures.loc[pre_trial_measures['STORE_NBR'] == trial_store, 'TOT_SALES'].sum() / \
                                   pre_trial_measures.loc[pre_trial_measures['STORE_NBR'] == control_store, 'TOT_SALES'].sum()

# Apply the scaling factor
measure_over_time_sales = measure_over_time.copy()
scaled_control_sales = measure_over_time_sales.loc[measure_over_time_sales['STORE_NBR'] == control_store, :].copy()
scaled_control_sales['controlSales'] = scaled_control_sales['TOT_SALES'] * scaling_factor_for_control_sales

# Calculate the percentage difference between scaled control sales and trial sales
percentage_diff = pd.merge(scaled_control_sales[['YEARMONTH', 'controlSales']],
                           measure_over_time.loc[measure_over_time['STORE_NBR'] == trial_store, ['TOT_SALES', 'YEARMONTH']],
                           on='YEARMONTH')
percentage_diff['percentageDiff'] = np.abs(percentage_diff['controlSales'] - percentage_diff['TOT_SALES']) / percentage_diff['controlSales']

# As our null hypothesis is that the trial period is the same as the pre-trial period,
# take the standard deviation based on the scaled percentage difference in the pre-trial period
std_dev = percentage_diff.loc[percentage_diff['YEARMONTH'] < 201902, 'percentageDiff'].std()
degrees_of_freedom = 7  # note that there are 8 months in the pre-trial period, hence 8 - 1 = 7 degrees of freedom

# Calculate t-values for the trial period
percentage_diff['tValue'] = (percentage_diff['percentageDiff'] - 0) / std_dev
percentage_diff['TransactionMonth'] = pd.to_datetime(dict(year=percentage_diff['YEARMONTH'] // 100, month=percentage_diff['YEARMONTH'] % 100, day=1))
print(percentage_diff.loc[(percentage_diff['YEARMONTH'] < 201905) & (percentage_diff['YEARMONTH'] > 201901), ['TransactionMonth', 'tValue']])

# Find the 95th percentile of the t distribution with the appropriate degrees of freedom to compare against
from scipy.stats import t
print(f"95th percentile of t distribution with {degrees_of_freedom} degrees of freedom: {t.ppf(0.95, df=degrees_of_freedom)}")

# Trial and control store total sales
measure_over_time_sales = measure_over_time.copy()
past_sales = measure_over_time_sales.assign(Store_type=lambda x: np.where(x['STORE_NBR'] == trial_store, 'Trial',
                                                                           np.where(x['STORE_NBR'] == control_store, 'Control', 'Other stores')))
past_sales = past_sales.loc[past_sales['Store_type'].isin(['Trial', 'Control']), ['YEARMONTH', 'Store_type', 'TOT_SALES']].groupby(['YEARMONTH', 'Store_type'])['TOT_SALES'].mean().reset_index()
past_sales['TransactionMonth'] = pd.to_datetime(dict(year=past_sales['YEARMONTH'] // 100, month=past_sales['YEARMONTH'] % 100, day=1))

# Control store 95th percentile
past_sales_controls95 = past_sales.loc[past_sales['Store_type'] == 'Control', :].copy()
past_sales_controls95['TOT_SALES'] = past_sales_controls95['TOT_SALES'] * (1 + std_dev * 2)
past_sales_controls95['Store_type'] = 'Control 95th % confidence interval'

# Control store 5th percentile
past_sales_controls5 = past_sales.loc[past_sales['Store_type'] == 'Control', :].copy()
past_sales_controls5['TOT_SALES'] = past_sales_controls5['TOT_SALES'] * (1 - std_dev * 2)
past_sales_controls5['Store_type'] = 'Control 5th % confidence interval'

trial_assessment = pd.concat([past_sales, past_sales_controls95, past_sales_controls5], ignore_index=True)

# Plotting these in one nice graph
plt.figure(figsize=(10, 6))
for store_type, group in trial_assessment.groupby('Store_type'):
    plt.plot(group['TransactionMonth'], group['TOT_SALES'], linestyle='-' if store_type == 'Trial' else '--', label=store_type)
plt.axvspan(datetime(2019, 2, 1), datetime(2019, 5, 1), alpha=0.3, color='gray')
plt.title('Total sales by month')
plt.xlabel('Month of operation')
plt.ylabel('Total sales')
plt.legend()
plt.show()
# Scale pre-trial control customers to match pre-trial trial store customers
scaling_factor_for_control_cust = pre_trial_measures.loc[pre_trial_measures['STORE_NBR'] == trial_store, 'LYLTY_CARD_NBR'].sum() / \
                                  pre_trial_measures.loc[pre_trial_measures['STORE_NBR'] == control_store, 'LYLTY_CARD_NBR'].sum()

# Apply the scaling factor
measure_over_time_custs = measure_over_time.copy()
scaled_control_customers = measure_over_time_custs.loc[measure_over_time_custs['STORE_NBR'] == control_store, :].copy()
scaled_control_customers['controlCustomers'] = scaled_control_customers['LYLTY_CARD_NBR'] * scaling_factor_for_control_cust

# Create a new column 'Store_type' based on conditions
scaled_control_customers['Store_type'] = np.where(scaled_control_customers['STORE_NBR'] == trial_store, 'Trial',
                                                  np.where(scaled_control_customers['STORE_NBR'] == control_store, 'Control', 'Other stores'))

# Calculate the percentage difference between scaled control sales and trial sales
percentage_diff = pd.merge(scaled_control_customers[['YEARMONTH', 'controlCustomers']],
                           measure_over_time.loc[measure_over_time['STORE_NBR'] == trial_store, ['LYLTY_CARD_NBR', 'YEARMONTH']],
                           on='YEARMONTH')
percentage_diff['percentageDiff'] = np.abs(percentage_diff['controlCustomers'] - percentage_diff['LYLTY_CARD_NBR']) / percentage_diff['controlCustomers']

# As our null hypothesis is that the trial period is the same as the pre-trial period,
# take the standard deviation based on the scaled percentage difference in the pre-trial period
std_dev = percentage_diff.loc[percentage_diff['YEARMONTH'] < 201902, 'percentageDiff'].std()
degrees_of_freedom = 7  # note that there are 8 months in the pre-trial period, hence 8 - 1 = 7 degrees of freedom

# Trial and control store number of customers
past_customers = scaled_control_customers.loc[scaled_control_customers['Store_type'].isin(['Trial', 'Control']), ['YEARMONTH', 'Store_type', 'LYLTY_CARD_NBR']]